{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Obsidian Notes","text":"<p>Publish your public notes with MkDocs</p>"},{"location":"#hello-world","title":"Hello World!","text":"<p>The <code>index.md</code> in the <code>/docs</code> folder is the homepage you see here.</p> <p>The folders in <code>/docs</code> appear as the main sections on the navigation bar.</p> <p>The notes appear as pages within these sections. For example, Note 1 in <code>Topic 1</code></p>"},{"location":"Features/LaTeX%20Math%20Support/","title":"LaTeX Math Support","text":"<p>LaTeX math is supported using MathJax.</p> <p>Inline math looks like \\(f(x) = x^2\\). The input for this is <code>$f(x) = x^2$</code>. Use <code>$...$</code>.</p> <p>For a block of math, use <code>$$...$$</code> on separate lines</p> <pre><code>$$\nF(x) = \\int^a_b \\frac{1}{2}x^4\n$$\n</code></pre> <p>gives </p> \\[ F(x) = \\int^a_b \\frac{1}{2}x^4 \\]"},{"location":"Features/Mermaid%20Diagrams/","title":"Mermaid diagrams","text":"<p>Here's the example from MkDocs Material documentation: </p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"Features/Text%20Formatting/","title":"Text Formatting","text":"<p>You can have lists like this</p> <ul> <li>first</li> <li>second</li> <li>third</li> </ul> <p>Or checklist lists to</p> <ul> <li> Get</li> <li> things</li> <li> done</li> </ul> <p>Also, get highlights and strikethroughs as above (similar to Obsidian).</p> <p>More formatting options for your webpage here. (but not compatible with Obsidian)</p>"},{"location":"Topic%201/Note%201/","title":"Note 1","text":"<p>Example: link to Mermaid Diagrams under <code>Features</code></p>"},{"location":"Topic%201/Note%202/","title":"Note 2","text":""},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/12/31/intro-why-parallel-computing/","title":"Intro. Why parallel computing?","text":"<p>Before we get our feet wet with parallel computing, I would like to preface with a simple question- why parallel computing? The answer is fairly simple: transistor scaling is plateau-ing. The following excerpt from New York Times (2004!) sheds more light on this pivotal moment:</p> <p>Intel's Big Shift After Hitting Technical Wall:</p> <p>...Then two weeks ago, Intel, the world's largest chip maker, publicly acknowledged that it had hit a \"thermal wall\" on its microprocessor line. As a result, the company is changing its product strategy and disbanding one of its most advanced design groups. Intel also said that it would abandon two advanced chip development projects, code- named Tejas and Jayhawk. Now, Intel is embarked on a course already adopted by some of its major rivals: obtaining more computing power by stamping multiple processors on a single chip rather than straining to increase the speed of a single processor....</p> <p>- John Markoff, New York Times, May 17, 2004</p> <p>The ever-famous Moore's Law set a standard when reducing CMOS transistors size, pushing fabrication factories to halve the size of its chip every year. Smaller and faster transistors led to higher clock rate, lower power consumption, and economical use of chip area. Advanced compilers and vendor-independent OS (e.g., Linux) also lowered the cost in bringing out new architectures. With single-threaded CPU performance doubling every 18 months, working to parallelize program code was often not worth the time.</p> <p>However, the glorious era of CPUs come to an end when limitations surface from the two driving forces of CPU performance improvement.</p> <ul> <li>Diminishing gains with instruction-level parallelism (ILP): It is difficult to increase issue width (i.e., the number of instructions a processor can execute simultaneously), and benefits gleaned from this declines. </li> <li>Power wall: It is difficult to push SOTA clock frequency due to high temperature (Frequency \\(\\propto\\) Power consumption \\(\\propto\\) Temperature).</li> </ul> <p>Thus began the era of parallel computing. </p>","tags":["notes"]},{"location":"blog/2024/12/09/triton-practitioners-guide/","title":"Triton practitioners guide","text":"<p>Notes for Lecture 14: Practitioners Guide to Triton</p> <p> indicates that code snippet is included in the attached notebook. </p>","tags":["notes"]},{"location":"blog/2024/12/09/triton-practitioners-guide/#is-your-ai-model-not-fast-enough","title":"Is your AI model not fast enough?","text":"<ol> <li>Not fast enough? <code>torch.compile</code>\u00a0it.<ul> <li><code>torch.compile</code>\u00a0makes your model faster by trying to use existing kernels more effectively and creating simple new (Triton) kernels. </li> </ul> </li> <li>Rewrite your code to make it more suitable for\u00a0<code>torch.compile</code>.</li> <li>Check which parts are slow and write custom Triton kernel(s) for those.</li> <li>\bCheck which parts are slow and write custom CUDA kernel(s) for those.</li> </ol>","tags":["notes"]},{"location":"blog/2024/12/09/triton-practitioners-guide/#how-to-write-triton-kernels","title":"How to write Triton kernels","text":"<ul> <li>Debug Triton kernels: Set environment variable <code>TRITON_INTERPRET = 1</code>   \u2192 Triton runs on the CPU, but simulates that it runs on the GPU. </li> <li>Utility functions for debugging: <ul> <li><code>check_tensors_gpu_ready</code>: (i) assert all tensors are contiguous in memory and (ii) assert all tensors are on gpu (when not simulating)</li> <li><code>breakpoint_if</code>: set a breakpoint, depending on conditions on pids</li> <li><code>print_if</code>:\u00a0print sth, depending on conditions on pids</li> </ul> </li> </ul>","tags":["notes"]},{"location":"blog/2024/12/09/triton-practitioners-guide/#programming-model","title":"Programming model","text":"<ul> <li>CUDA computation unit: <code>block &gt; thread</code><ul> <li>All threads in a block run on the same SM and share the same Shared Memory.</li> <li>Each thread computes on scalars. </li> </ul> </li> <li>Triton computation unit: <code>block</code><ul> <li>Can't manage shared memory directly. Triton does that automatically.</li> <li>All operations perform on vectors. </li> </ul> </li> <li>Jargon: In triton lingo, each kernel is called a \"program\". Similarly, \"block_id\" is called \"pid\". </li> <li>Example: Add\u00a0<code>x</code>\u00a0and\u00a0<code>y</code>(vector size: 6) and save the output into\u00a0<code>z</code> . Block size is 4, so we have\u00a0<code>cdiv(6, 4) = 2</code>\u00a0blocks.</li> </ul> cuda <pre><code># x,y = input tensors, z = output tensors, n = size of x, bs = block size\ndef add_cuda_k(x, y, z, n, bs):\n    # locate which part of the computation this specific kernel is doing\n    block_id = ... # in our example: one of [0,1] \n    thread_id = ... # in our example: one of [0,1,2,3] \n\n    # identify the location of the data this specific kernel needs\n    offs = block_id * bs + thread_id\n\n    # guard clause, to make sure we're not going out of bounds\n    if offs &lt; n:\n        # read data\n        x_value = x[offs]\n        y_value = y[offs]\n\n        # do operation\n        z_value = x_value + y_value\n\n        # write data\n        z[offs] = z_value\n\n    # Important: offs, x_value, y_value, x_value are all scalars!\n    # The guard clause is also (kind of) scalar op.\n</code></pre> triton <pre><code># Note: this is for illustration, and not quite syntactically correct. See further below for correct triton syntax\n\ndef add_triton_k(x, y, z, n, bs):\n    # locate which part of the computation this specific kernel is doing\n    block_id = tl.program_id(0)  # in our example: one of [0,1] \n\n    # identify the location of the data this specific kernel needs\n    offs = block_id * bs + tl.arange(0, bs) # &lt;- this is a vector!\n\n    # the guard clause becomes a mask, which is a vector of bools\n    mask = offs &lt; n # &lt;- this is a vector of bools!\n\n    # read data\n    x_values = x[mask] # &lt;- a vector is read!\n    y_values = y[mask] # &lt;- a vector is read!\n\n    # do operation\n    z_value = x_value + y_value  # &lt;- vectors are added!\n\n    # write data\n    z[offs] = z_value  # &lt;- a vector is written!\n</code></pre>","tags":["notes"]},{"location":"blog/2024/12/09/triton-practitioners-guide/#example-1-copying-a-tensor","title":"Example 1: Copying a tensor","text":"","tags":["notes"]},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/parallel-computing/","title":"parallel-computing","text":""},{"location":"blog/category/triton/","title":"triton","text":""}]}